{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контрольная работа"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "file_path = 'preprocessed_descriptions.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "descriptions = df['description'].tolist()\n",
    "\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "for description in descriptions:\n",
    "    tokens = word_tokenize(description)\n",
    "    unique_words.update(tokens)\n",
    "\n",
    "\n",
    "print(unique_words)\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import random\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "file_path = 'preprocessed_descriptions.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "descriptions = df['description'].tolist()\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "for description in descriptions:\n",
    "    tokens = word_tokenize(description)\n",
    "    unique_words.update(tokens)\n",
    "\n",
    "unique_words_list = list(unique_words)\n",
    "\n",
    "random_pairs = [(random.choice(unique_words_list), random.choice(unique_words_list)) for _ in range(5)]\n",
    "\n",
    "distances = [(word1, word2, edit_distance(word1, word2)) for word1, word2 in random_pairs]\n",
    "\n",
    "for word1, word2, dist in distances:\n",
    "    print(f\"Слова: {word1}, {word2} -> Расстояние редактирования: {dist}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bake', 1), ('make', 1), ('lake', 1)]\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from nltk.metrics import edit_distance\n",
    "from typing import List, Tuple\n",
    "\n",
    "def k_nearest_words(word: str, words: List[str], k: int) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Возвращает k ближайших слов к заданному слову из списка, используя расстояние Левенштейна.\n",
    "\n",
    "    :param word: Заданное слово\n",
    "    :param words: Список слов\n",
    "    :param k: Количество ближайших слов, которые нужно вернуть\n",
    "    :return: Список из k ближайших слов и их расстояний в виде кортежей (слово, расстояние)\n",
    "    \"\"\"\n",
    "    \n",
    "    distances = [(w, edit_distance(word, w)) for w in words]\n",
    "    \n",
    "    \n",
    "    k_nearest = heapq.nsmallest(k, distances, key=lambda x: x[1])\n",
    "    \n",
    "    return k_nearest\n",
    "\n",
    "# Пример со словами\n",
    "word = \"cake\"\n",
    "words = [\"bake\", \"make\", \"lake\", \"rake\", \"take\", \"fake\", \"stake\", \"snack\", \"break\", \"quake\"]\n",
    "k = 3\n",
    "\n",
    "nearest_words = k_nearest_words(word, words, k)\n",
    "print(nearest_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "file_path = 'preprocessed_descriptions.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "descriptions = df['description'].tolist()\n",
    "\n",
    "unique_words = set()\n",
    "for description in descriptions:\n",
    "    tokens = word_tokenize(description)\n",
    "    unique_words.update(tokens)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data = []\n",
    "for word in unique_words:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    normalized_word = lemmatizer.lemmatize(word)\n",
    "    data.append((word, stemmed_word, normalized_word))\n",
    "\n",
    "df_words = pd.DataFrame(data, columns=['word', 'stemmed_word', 'normalized_word'])\n",
    "df_words.set_index('word', inplace=True)\n",
    "\n",
    "print(df_words.head())\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Word: {df_words.index[i]}, Stemmed: {df_words.iloc[i, 0]}, Lemmatized: {df_words.iloc[i, 1]}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
